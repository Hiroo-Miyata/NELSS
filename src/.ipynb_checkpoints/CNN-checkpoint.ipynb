{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "114eb1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# get filename_list\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd851efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('../data/preprocessed/hiroo-cnn.mat')\n",
    "## load\n",
    "eeg = mat[\"eegData\"]\n",
    "emotions = mat[\"labels\"]\n",
    "emotions = emotions.astype(int)\n",
    "nchannel = eeg.shape[1]\n",
    "ntrial = eeg.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e18db335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = emotions.astype(int)\n",
    "emotions.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707c6470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8871f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataAugmentation(matrix, labels, DATAPOINT, STRIDE_SIZE, fs): ## able to improve\n",
    "    n_size = int((matrix.shape[0] - DATAPOINT) // STRIDE_SIZE) + 1\n",
    "    augmentedMat = np.empty((n_size*matrix.shape[2], 1, DATAPOINT, matrix.shape[1]))\n",
    "    augmentedLabel = np.empty((n_size*matrix.shape[2]))\n",
    "    for t in range(matrix.shape[-1]):\n",
    "        trial = matrix[:, :, t]\n",
    "        for i in range(n_size):\n",
    "            augmentedMat[i+t*n_size, :, :, :] = trial[i*STRIDE_SIZE:i*STRIDE_SIZE+DATAPOINT, :]\n",
    "            endpoint = (i*STRIDE_SIZE+DATAPOINT)\n",
    "            if  endpoint <= 10*fs: \n",
    "                augmentedLabel[i+t*n_size] = labels[t, 0] - 1\n",
    "            elif  endpoint >= 60*fs: \n",
    "                augmentedLabel[i+t*n_size] = labels[t, 5] - 1\n",
    "            else:\n",
    "                idx = endpoint // (10*fs)\n",
    "                position =  (endpoint - idx*10*fs)/(10*fs)\n",
    "                estLabel = (labels[t, idx] - labels[t, idx-1]) * position + labels[t, idx-1] - 1\n",
    "                if np.round(estLabel)  > 4:\n",
    "                    print(t, i, np.round(estLabel), idx, position, (labels[t, idx] - labels[t, idx-1]), labels[t, idx-1], labels[t, :])\n",
    "                augmentedLabel[i+t*n_size] =  np.round(estLabel) \n",
    "    return augmentedMat, augmentedLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44892928",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1232, 1, 640, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, train_y = dataAugmentation(eeg, emotions, 640, 128, 128)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6216a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy import stats\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class EmotionDataManager(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.df = data\n",
    "        self.label = label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        label = torch.tensor(self.label[index]).long()\n",
    "        data = torch.tensor(self.df[index]).float()\n",
    "        \n",
    "        return data, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47942556",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        ## (1, 640, 28)\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, 16, kernel_size=(16, 8), stride=(4,4), padding=(6, 2)),\n",
    "                                   nn.BatchNorm2d(16),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.MaxPool2d(1)\n",
    "                                  )\n",
    "        ## (16, 160, 7)\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16, 32, kernel_size=(16,3) , stride=(4,3), padding=(6, 1)),\n",
    "                                   nn.BatchNorm2d(32),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.MaxPool2d(1),\n",
    "                                  )\n",
    "        ## (32, 40, 3)\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(32, 64, kernel_size=(9,3) , stride=(1,1), padding=(0, 0)),\n",
    "                                   nn.BatchNorm2d(64),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.MaxPool2d(1),\n",
    "                                  )\n",
    "        ## (64,32,1)\n",
    "        self.dropout1 = torch.nn.Dropout(p=0.50)\n",
    "#         self.dropout2 = torch.nn.Dropout(p=0.50)\n",
    "        self.fc1 = nn.Sequential(nn.Linear(64 * 32 *  1, 128),nn.ReLU(inplace=True),)\n",
    "#         self.fc2 = nn.Sequential(nn.Linear(1024, 64),nn.ReLU(inplace=True),)\n",
    "        self.fc3 = nn.Linear(128, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc1(x)\n",
    "        x= self.dropout1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x= self.dropout2(x)\n",
    "#         print(x[:, :5])\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a3f049b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(910, 1, 640, 29)\n",
      "epoch 1\n",
      "train mean loss=1.8571417939977561, accuracy=0.17595307917888564\n",
      "test  mean loss=1.6848002818592809, accuracy=0.25\n",
      "epoch 2\n",
      "train mean loss=1.795924944611938, accuracy=0.28005865102639294\n",
      "test  mean loss=1.6555193265279133, accuracy=0.32894736842105265\n",
      "epoch 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m             inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     47\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 48\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#             print(\"===============\")\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#             print(i)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#             print(\"===============\")\u001b[39;00m\n\u001b[1;32m     52\u001b[0m             loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:446\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:442\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    440\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    441\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Input\n",
    "fs = 128\n",
    "tw = 5\n",
    "DATAPOINT = fs*tw\n",
    "STRIDE_SIZE = fs//2\n",
    "batch_size = 16\n",
    "n_splits = 4\n",
    "studyRate = 0.00001\n",
    "kf = KFold(n_splits=n_splits)\n",
    "description = \"benny-bigkernel-\" +str(DATAPOINT)+\"-\"+str(STRIDE_SIZE)+\"-lr-\"+str(studyRate)\n",
    "\n",
    "X=range(ntrial)\n",
    "trail = 10\n",
    "\n",
    "train_loss_value=np.empty((n_splits, trail))\n",
    "train_acc_value=np.empty((n_splits, trail))\n",
    "test_loss_value=np.empty((n_splits, trail))\n",
    "test_acc_value=np.empty((n_splits, trail))\n",
    "test_acc_each=np.zeros((n_splits, trail, 5))\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    train_x, train_y = dataAugmentation(eeg[:, :, train_index], emotions[train_index, :], DATAPOINT, STRIDE_SIZE, fs)\n",
    "    test_x, test_y = dataAugmentation(eeg[:, :, test_index], emotions[test_index, :], DATAPOINT, STRIDE_SIZE, fs)\n",
    "    traindataset = EmotionDataManager(train_x, train_y)\n",
    "    testdataset = EmotionDataManager(test_x, test_y)\n",
    "    trainloader = DataLoader(traindataset, batch_size,shuffle=True, num_workers=0)\n",
    "    testloader = DataLoader(testdataset, batch_size,shuffle=True, num_workers=0)\n",
    "\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net = CNN()\n",
    "    model = net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=studyRate)\n",
    "    for epoch in range(trail):\n",
    "        print('epoch', epoch+1)\n",
    "\n",
    "        sum_loss = 0.0\n",
    "        sum_correct = 0\n",
    "        sum_total = 0\n",
    "\n",
    "        net.train()\n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "#             print(\"===============\")\n",
    "#             print(i)\n",
    "#             print(\"===============\")\n",
    "            loss = criterion(outputs, labels)\n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum(p.pow(2.0).sum()\n",
    "                          for p in model.parameters())\n",
    "\n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ## softmaxをかませる\n",
    "            sum_loss += loss.item()                            \n",
    "            _, predicted = outputs.max(1)                      \n",
    "            sum_total += labels.size(0)                 \n",
    "            sum_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(\"train mean loss={}, accuracy={}\".format(sum_loss*batch_size/len(trainloader.dataset), float(sum_correct/sum_total)))\n",
    "        train_loss_value[fold, epoch] = sum_loss*batch_size/len(trainloader.dataset)\n",
    "        train_acc_value[fold, epoch] = float(sum_correct/sum_total)\n",
    "\n",
    "        sum_loss = 0.0\n",
    "        sum_correct = 0\n",
    "        sum_total = 0\n",
    "        \n",
    "        label_each = np.zeros(5)\n",
    "        acc_each = np.zeros(5)\n",
    "\n",
    "        net.eval()\n",
    "        for i, (inputs, labels) in enumerate(testloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            sum_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            sum_total += labels.size(0)\n",
    "            sum_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            for l, lab in enumerate(labels):\n",
    "                label_each[lab] += 1\n",
    "                if (predicted[l] == lab):\n",
    "                    acc_each[lab] += 1\n",
    "\n",
    "        print(\"test  mean loss={}, accuracy={}\".format(sum_loss*batch_size/len(testloader.dataset), float(sum_correct/sum_total)))\n",
    "        test_loss_value[fold, epoch] = sum_loss*batch_size/len(testloader.dataset)\n",
    "        test_acc_value[fold, epoch] = float(sum_correct/sum_total)\n",
    "        for lab in range(5):\n",
    "            if  label_each[lab] > 0:\n",
    "                test_acc_each[fold, epoch, lab] = acc_each[lab] / label_each[lab]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a9969f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3757, 1, 640, 28) 3757 235\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, len(traindataset), len(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "657e963c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 4, 2, 2, 1, 1, 2, 1, 2, 1, 2, 2, 1, 2, 5])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "a3a15886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False,  True])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y > 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "ea5afc51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 3, 3, 3, 3, 3],\n",
       "       [3, 4, 4, 4, 4, 4],\n",
       "       [2, 2, 2, 2, 3, 2],\n",
       "       [2, 2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2, 2],\n",
       "       [3, 3, 3, 3, 3, 3],\n",
       "       [3, 3, 3, 3, 3, 3],\n",
       "       [2, 2, 2, 2, 2, 2],\n",
       "       [3, 3, 3, 4, 4, 4],\n",
       "       [2, 2, 2, 2, 2, 2],\n",
       "       [3, 3, 3, 3, 3, 3],\n",
       "       [2, 2, 2, 3, 3, 3],\n",
       "       [4, 4, 4, 4, 4, 4],\n",
       "       [2, 2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 1, 1, 1],\n",
       "       [4, 4, 4, 5, 5, 5]])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[train_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "de9f8c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 640, 28])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "e2f5b175",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([2.1957, 3.6326, 2.2578, 1.6731, 4.7634, 3.4991, 4.2598, 2.2290, 1.9866,\n",
       "        1.5828, 1.9016, 3.5990, 2.3012, 1.7210, 2.0013, 2.5262],\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([1, 1, 1, 2, 2, 2, 2, 1, 1, 0, 1, 1, 1, 3, 2, 2]))"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "b2af4ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 1, 2, 3])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce5c19f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jh/nkpm2xxn2qdbzyljrfx984m40000gn/T/ipykernel_13238/3456356805.py:28: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/var/folders/jh/nkpm2xxn2qdbzyljrfx984m40000gn/T/ipykernel_13238/3456356805.py:28: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/var/folders/jh/nkpm2xxn2qdbzyljrfx984m40000gn/T/ipykernel_13238/3456356805.py:28: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/var/folders/jh/nkpm2xxn2qdbzyljrfx984m40000gn/T/ipykernel_13238/3456356805.py:28: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_splits):\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "    fig = plt.figure(figsize=(30, 10))\n",
    "    ax_right = fig.add_subplot(121)\n",
    "    ax_left = fig.add_subplot(122)\n",
    "\n",
    "    x = np.linspace(0, test_loss_value.shape[1], test_loss_value.shape[1], endpoint=False)\n",
    "    ax_right.plot(x, train_loss_value[i, :], label=\"train\")\n",
    "    ax_right.plot(x, test_loss_value[i, :], label=\"test\")\n",
    "    ax_right.set_title(\"Loss at each epoch\",fontsize=32)\n",
    "    ax_right.set_xlabel(\"Epoch\",fontsize=32)\n",
    "    ax_right.set_ylabel(\"Loss\",fontsize=32)\n",
    "    ax_right.tick_params(axis='x', labelsize=24)\n",
    "    ax_right.tick_params(axis='y', labelsize=24)\n",
    "    ax_right.legend()\n",
    "\n",
    "\n",
    "    ax_left.plot(x, train_acc_value[i, :], label=\"train\")\n",
    "    ax_left.plot(x, test_acc_value[i, :], label=\"test\")\n",
    "    ax_left.set_title(\"Accuracy at each epoch\",fontsize=32)\n",
    "    ax_left.set_xlabel(\"Epoch\",fontsize=32)\n",
    "    ax_left.set_ylabel(\"Accuracy rate\",fontsize=32)\n",
    "    ax_left.tick_params(axis='x', labelsize=24)\n",
    "    ax_left.tick_params(axis='y', labelsize=24)\n",
    "    ax_left.legend()\n",
    "\n",
    "    plt.show()\n",
    "    plt.savefig('../results/hiroo/cnn/1127-%s-fold-%d.jpg' % (description, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fef002d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
